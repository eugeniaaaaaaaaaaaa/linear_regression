{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNyxgFtLqkvmOl/AjFVQHhK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eugeniaaaaaaaaaaaa/linear_regression/blob/main/%D0%98%D1%82%D0%BE%D0%B3%D0%BE%D0%B2%D1%8B%D0%B9_%D0%BF%D1%80%D0%BE%D0%B5%D0%BA%D1%82_DA%26ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Кейс «Анализ факторов, влияющих на употребление табака в США»\n",
        "**Аннотация.** Курение, несмотря на все существующие предупреждения о его вредности, продолжает оставаться одной из самых распространенных вредных привычек среди населения США. Каждый год тысячи людей умирают от заболеваний, связанных с курением. Изучение причин курения и поиски эффективных способов борьбы с этой привычкой являются важными задачами в области общественного здравоохранения. Программы по борьбе с курением, включающие информационные кампании, консультации и другие меры, могут помочь людям бросить курить и избежать многих заболеваний, связанных с курением. Поэтому изучение данных об употреблении табака и поиск способов борьбы с курением являются важными шагами на пути к более здоровому обществу.\n",
        "\n",
        "**Цель:** проанализировать данные по употреблению табака в США. Выявить основные факторы, влияющие на уровень употребления табака и предложить прогноз (построить регрессионную модель) по уровню употреблению табака в США в зависимости от выявленных факторов.\n",
        "\n",
        "**Техническое задание:** требуется проанализировать данные по употреблению табака в США и зависимости количества людей, курящих ежедневно, от таких факторов как год, штат, процент бросивших курить, процент никогда не курящих и процент людей, иногда употребляющих табак, а также дать прогноз по употреблению табака в США. Построить качественную модель и объяснить ее выбор. По построенной модели дать интерпретацию (какие факторы влияют на вероятность курения или бросания курения и определить наиболее важные факторы, которые можно использовать для разработки эффективных программ по борьбе с курением) и дать прогноз по будущим тенденциям и изменениям употребления табака в США. Пояснить полученные результаты.\n"
      ],
      "metadata": {
        "id": "MxHlCDb5YBgu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "В датасете есть полные данные (16 - c 1995 по 2010) по 50 штатам, District of Columbia - 15 (с 1996), Puerto Rico - 15(с 1996), Hawaii - 15(без 2004), Utah - 14(с 1997), Virgin Islands - 10 (с 2001), Guam - 7 (с 2001 кроме 2004). Пуэрто-Рико,  Виргинские Острова, Гуам - островные территории.\n",
        "\n",
        "Еще есть общие полные данные - Nationwide (States, DC, and Territories) и Nationwide (States and DC)"
      ],
      "metadata": {
        "id": "crPGGSWuJIuO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZyyZG7Q9IVff"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import statsmodels.stats.api as sms\n",
        "import statsmodels.api as sm\n",
        "import sklearn.metrics as metrics\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def chaddock(_corrcoef):\n",
        "  _corr = abs(_corrcoef)\n",
        "  _vals = ((0.1, 'отсутствует'), (0.3, 'слабая'), (0.5, 'умеренная'),\n",
        "           (0.7, 'заметная'), (0.9, 'высокая'), (1, 'очень высокая'))\n",
        "  _ans = 'коэффициент корреляции вычислен неверно'\n",
        "  for _v in _vals:\n",
        "    if _corr <= _v[0]:\n",
        "      _ans = _v[1]\n",
        "      break\n",
        "  return _ans\n",
        "\n",
        "def r2_adj(X, y, y_count):\n",
        "  n = len(y)  # объем выборки, на которой строили модель\n",
        "  k = len(X) + 1  # количество параметров модели\n",
        "  return 1 - (1 - metrics.r2_score(y, y_count)) * (n - 1) / (n - k)"
      ],
      "metadata": {
        "id": "0T2Xd7aCBvN-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://github.com/eugeniaaaaaaaaaaaa/linear_regression/blob/main/Smoking.csv', sep=';')\n",
        "df = df.drop('index', axis=1)\n",
        "\n",
        "df.columns = ['year', 'state', 'everyday', 'sometimes', 'former', 'never']\n",
        "smokers_data = ['everyday', 'sometimes', 'former', 'never']\n",
        "names = {\n",
        "    'year': 'Год',\n",
        "    'state': 'Название штата',\n",
        "    'everyday': 'Курящие каждый день',\n",
        "    'sometimes': 'Курящие иногда',\n",
        "    'former': 'Бросившие курить',\n",
        "    'never': 'Никогда не курившие'\n",
        "}\n",
        "\n",
        "print(f'Количество пропусков в данных:\\n{df.isna().sum()}')\n",
        "\n",
        "df[smokers_data] = df[smokers_data].applymap(lambda v: float(v[:-1]) / 100)\n",
        "df_common = df[df.state == 'Nationwide (States, DC, and Territories)']\n",
        "df_no_ter =  df[df.state =='Nationwide (States and DC)']\n",
        "df = df[~((df.state == 'Nationwide (States, DC, and Territories)') | (df.state =='Nationwide (States and DC)'))]#.sort_values(by=['Year'], ascending=True)\n",
        "\n",
        "print('''\n",
        "Пропусков в строках данных нет, однако в датасете отсутствуют следующие данные:\n",
        "- District of Columbia (1995)\n",
        "- Puerto Rico (1995)\n",
        "- Hawaii (2004)\n",
        "- Utah (1995-1996)\n",
        "- Virgin Islands (1995-2000)\n",
        "- Guam (1995-2000, 2004)\n",
        "''')\n",
        "\n",
        "print(f'''\n",
        "Проверка корректности данных - суммируем доли опрошенных\n",
        "Основной датасет: {df[smokers_data].sum(axis=1).mean():.3f}\n",
        "Nationwide (States, DC, and Territories): {df_common[smokers_data].sum(axis=1).mean():.3f}\n",
        "Nationwide (States and DC): {df_no_ter[smokers_data].sum(axis=1).mean():.3f}\n",
        "0.7% вероятно потеряны из-за округления ''')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        },
        "id": "ksEFiD5tInpO",
        "outputId": "df1e14cb-d3df-4331-db89-aa33e4a93cba"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ParserError",
          "evalue": "Error tokenizing data. C error: Expected 1 fields in line 35, saw 2\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-171186996.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://github.com/eugeniaaaaaaaaaaaa/linear_regression/blob/main/Smoking.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m';'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'index'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'state'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'everyday'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sometimes'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'former'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'never'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msmokers_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'everyday'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sometimes'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'former'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'never'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1921\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1922\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1924\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1925\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 35, saw 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# переменную State превратим в дамми-переменные\n",
        "dummy = pd.get_dummies(\n",
        "    df.state,\n",
        "    drop_first=False\n",
        ")\n",
        "df = pd.concat([df, dummy], axis=\"columns\")\n",
        "df.sample(5, random_state=7)"
      ],
      "metadata": {
        "id": "Kshv9AWQbGbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Среднее:')\n",
        "print(df[smokers_data].mean())\n",
        "\n",
        "print('\\nМода:')\n",
        "print(df[smokers_data].mode())\n",
        "\n",
        "print('\\nМедиана:')\n",
        "print(df[smokers_data].median())\n",
        "\n",
        "print('\\nСтандартное отклонение:')\n",
        "print(df[smokers_data].std())\n",
        "\n",
        "\n",
        "print('\\nКвантили:')\n",
        "print(pd.concat([df[smokers_data].quantile(q=0.25), df[smokers_data].quantile(q=0.5), df[smokers_data].quantile(q=0.75)], axis=1, keys=['Q1', 'Q2', 'Q3']))"
      ],
      "metadata": {
        "id": "LNneNDN9gWTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[smokers_data].describe()[1:]"
      ],
      "metadata": {
        "id": "7yjwdeiYjESs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# проверка на нормальность\n",
        "plt.rcParams.update({'font.size': 20})\n",
        "plt.figure(figsize=(15, 3))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(df.year, bins=10, color=\"lightblue\", linewidth=1, edgecolor=\"black\")\n",
        "plt.xlabel(names[\"year\"] )\n",
        "plt.ylabel(\"Частота\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(df.state, bins=len(df.state.unique()), color=\"lightblue\", linewidth=1, edgecolor=\"black\", label='enabled')\n",
        "plt.xlabel(names[\"state\"])\n",
        "plt.ylabel(\"Частота\")\n",
        "plt.xticks([])\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(28, 5))\n",
        "\n",
        "plt.subplot(1, 4, 1)\n",
        "plt.hist(df[smokers_data[0]], bins=10, color=\"coral\", linewidth=1, edgecolor=\"black\")\n",
        "plt.xlabel(names[smokers_data[0]])\n",
        "plt.ylabel(\"Частота\")\n",
        "for i in range(1, 4):\n",
        "  plt.subplot(1, 4, i + 1)\n",
        "  plt.hist(df[smokers_data[i]], bins=10, color=\"coral\", linewidth=1, edgecolor=\"black\")\n",
        "  plt.xlabel(names[smokers_data[i]])\n"
      ],
      "metadata": {
        "id": "5cx_Qp_Bh_8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alpha = 0.05\n",
        "isnorm = lambda p_value: ('ненормально', 'нормально')[int(p_value >= alpha)]\n",
        "print(f'''Проверим данные на нормальность с помощью критерия Жарка-Бера.\n",
        "Уровень значимости α = 0.05\\n''')\n",
        "for col in smokers_data + ['year']:\n",
        "  p_val = stats.jarque_bera(df[col])[1]\n",
        "  print(f'{names[col]}:  p = {p_val:.3}, распределение {isnorm(p_val)}')"
      ],
      "metadata": {
        "id": "fYSs7dRRb_Hc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# проверим данные на упорядоченность\n",
        "# используем критерий Вальда-Волфовитца\n",
        "import statsmodels\n",
        "print(\"p-значение:\", statsmodels.sandbox.stats.runs.runstest_1samp(df.everyday)[1])\n",
        "print(\"p-значение после перемешивания:\", statsmodels.sandbox.stats.runs.runstest_1samp(df.sample(len(df), random_state=6786866).everyday)[1])\n",
        "df = df.sample(len(df), random_state=6786866)"
      ],
      "metadata": {
        "id": "LXVRse-l7-FY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corr_matrix = df.corr(method=\"spearman\", numeric_only=True)"
      ],
      "metadata": {
        "id": "LAqhes-GhWMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(50, 5))\n",
        "\n",
        "sns.heatmap(corr_matrix[:5], annot=True, fmt=\".2f\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HwFftj78nSg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_cols = [col for col in corr_matrix.columns if abs(corr_matrix[col]['everyday']) > 0.1] # хотя бы слабая корреляция с everyday\n",
        "corr_matrix = df[X_cols].corr(method=\"spearman\")\n",
        "\n",
        "\n",
        "plt.figure(figsize=(9, 8))\n",
        "\n",
        "sns.heatmap(corr_matrix[:], annot=True, fmt=\".1f\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "X_cols = corr_matrix['everyday'].sort_values(ascending=False, key=abs).keys()[1:]"
      ],
      "metadata": {
        "id": "NI139GGHn6kj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# линейная модель со всеми факторами\n",
        "\n",
        "y_col = 'everyday'\n",
        "X_train, X_test, y_train, y_test = train_test_split(df[X_cols], df[y_col],\n",
        "                                                    test_size=0.2, random_state=999)\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X=X_train, y=y_train)\n",
        "# print(f\"Коэффициенты модели: {model.coef_}, свободный член: {model.intercept_}\")\n",
        "\n",
        "y_model = model.predict(X_train)\n",
        "r2_train= metrics.r2_score(y_train, y_model)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "r2_test = metrics.r2_score(y_test, y_pred)\n",
        "print(f\"R2_train = {round(r2_train, 3)}, R2_test = {round(r2_test, 3)}\")\n",
        "\n",
        "r2_adj_train, r2_adj_test = r2_adj(X_cols, y_train, y_model), r2_adj(X_cols, y_test, y_pred)\n",
        "print(f\"R2_adj_train = {round(r2_adj_train, 3)}, R2_adj_test = {round(r2_adj_test, 3)}\")\n",
        "\n",
        "print('Модель', ('не переобучена', 'переобучена')[int(r2_adj_train - r2_adj_test > 0.1)])\n",
        "\n",
        "# вычислим метрики MAE и RMSE на тестовой выборке\n",
        "MAE = metrics.mean_absolute_error(y_test, y_pred)\n",
        "RMSE = metrics.mean_squared_error(y_test, y_pred) ** 0.5\n",
        "\n",
        "print(f\"MAE = {round(MAE, 3)}, RMSE = {round(RMSE, 3)}\")\n",
        "print('\\n\\nКоэффициенты при целевых признаках')\n",
        "print(*list(zip(X_cols, model.coef_)), sep='\\n')\n",
        "\n",
        "# проверка теоремы Гаусса-Маркова\n",
        "resid = y_train - y_model\n",
        "exog = X_train.copy()\n",
        "exog['const'] = np.ones(len(X_train))\n",
        "\n",
        "alpha = 0.05\n",
        "p_mist = stats.ttest_1samp(a=resid, popmean=0)[1]\n",
        "p_homo = sm.stats.diagnostic.het_breuschpagan(resid, exog)[1]\n",
        "dw = sm.stats.stattools.durbin_watson(resid)\n",
        "p_jb = stats.jarque_bera(resid)[1]\n",
        "print(f'''\\n\\n\n",
        "1) Значения факторных признаков не случайны - выполняется.\n",
        "2) Математическое ожидание ошибки модели равно 0: p={p_mist:.3f} - {('выполняется', 'не выполняется')[int(p_mist < alpha)]}.\n",
        "3) Ошибки модели гомоскедастичны (их дисперсия постоянна): p={p_homo:.3f} - {('выполняется', 'не выполняется')[int(p_mist < alpha)]}.\n",
        "4) Ошибки модели должны быть некоррелированы (независимы): DW={dw:.3f} - {('не выполняется', 'выполняется')[int(1.5 < dw < 2.5)]}.\n",
        "5) Ошибки модели должны иметь нормальное распределение: p={p_jb:.3f} - {('выполняется', 'не выполняется')[int(p_mist < alpha)]}.\n",
        "''')\n"
      ],
      "metadata": {
        "id": "IJxU-BSOhO-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "С каждым годом доля курящих ежедневно уменьшается на 3.7%, а при увеличении количества никогда не куривших людей на 1% доля курящих ежедневно уменьшается на 0.27. Кроме того доля ежедневно курящих меняется в зависимости от штата."
      ],
      "metadata": {
        "id": "dPXNK5sykmi4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_cols1 = [col for col in corr_matrix.columns if abs(corr_matrix[col]['everyday']) > 0.3] # умеренная корреляция с everyday\n",
        "corr_matrix1 = df[X_cols1].corr(method=\"spearman\")\n",
        "plt.figure(figsize=(4, 3))\n",
        "sns.heatmap(corr_matrix1[:], annot=True, fmt=\".1f\")\n",
        "# hmap = sns.heatmap(df.corr(numeric_only=True), annot=True, fmt=\".1f\")\n",
        "plt.show()\n",
        "\n",
        "X_cols1 = corr_matrix1['everyday'].sort_values(ascending=False, key=abs).keys()[1:]# линейная модель\n",
        "\n",
        "y_col = 'everyday'\n",
        "X_train1, X_test1, y_train, y_test = train_test_split(\n",
        "  df[X_cols1], df[y_col], test_size=0.2, random_state=999\n",
        ")\n",
        "\n",
        "model1 = LinearRegression()\n",
        "model1.fit(X=X_train1, y=y_train)\n",
        "# print(f\"Коэффициенты модели: {model.coef_}, свободный член: {model.intercept_}\")\n",
        "\n",
        "y_model1 = model1.predict(X_train1)\n",
        "r2_train1= metrics.r2_score(y_train, y_model1)\n",
        "\n",
        "y_pred1 = model1.predict(X_test1)\n",
        "r2_test1 = metrics.r2_score(y_test, y_pred1)\n",
        "print(f\"R2_train = {round(r2_train1, 3)}, R2_test = {round(r2_test1, 3)}\")\n",
        "\n",
        "r2_adj_train1, r2_adj_test1 = r2_adj(X_cols1, y_train, y_model1), r2_adj(X_cols1, y_test, y_pred1)\n",
        "print(f\"R2_adj_train = {round(r2_adj_train1, 3)}, R2_adj_test = {round(r2_adj_test1, 3)}\")\n",
        "\n",
        "print('Модель', ('не переобучена', 'переобучена')[int(r2_adj_train1 - r2_adj_test1 > 0.1)])\n",
        "\n",
        "# вычислим метрики MAE и RMSE на тестовой выборке\n",
        "MAE1 = metrics.mean_absolute_error(y_test, y_pred1)\n",
        "# в версии sklearn в Colab отсутствует готовая функция для RMSE\n",
        "RMSE1 = metrics.mean_squared_error(y_test, y_pred1) ** 0.5\n",
        "\n",
        "print(f\"MAE = {round(MAE1, 3)}, RMSE = {round(RMSE1, 3)}\")\n",
        "print('\\n\\nКоэффициенты при целевых признаках')\n",
        "print(*list(zip(X_cols1, model1.coef_)), sep='\\n')\n",
        "\n",
        "# проверка теоремы Гаусса-Маркова\n",
        "import statsmodels.stats.api as sms\n",
        "resid1= y_train - y_model1\n",
        "exog1 = X_train1.copy()\n",
        "exog1['const'] = np.ones(len(X_train1))\n",
        "\n",
        "alpha = 0.05\n",
        "p_mist1 = stats.ttest_1samp(a=resid1, popmean=0)[1]\n",
        "p_homo1 = sm.stats.diagnostic.het_breuschpagan(resid1, exog1)[1]\n",
        "dw1 = sm.stats.stattools.durbin_watson(resid1)\n",
        "p_jb1 = stats.jarque_bera(resid1)[1]\n",
        "print(f'''\\n\\n\n",
        "1) Значения факторных признаков не случайны - выполняется.\n",
        "2) Математическое ожидание ошибки модели равно 0: p={p_mist1:.3f} - {('выполняется', 'не выполняется')[int(p_mist1 < alpha)]}.\n",
        "3) Ошибки модели гомоскедастичны (их дисперсия постоянна): p={p_homo1:.3f} - {('выполняется', 'не выполняется')[int(p_mist1 < alpha)]}.\n",
        "4) Ошибки модели должны быть некоррелированы (независимы): DW={dw1:.3f} - {('не выполняется', 'выполняется')[int(1.5 < dw1 < 2.5)]}.\n",
        "5) Ошибки модели должны иметь нормальное распределение: p={p_jb1:.3f} - {('выполняется', 'не выполняется')[int(p_mist1 < alpha)]}.\n",
        "''')"
      ],
      "metadata": {
        "id": "YG6e25ih-84-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}